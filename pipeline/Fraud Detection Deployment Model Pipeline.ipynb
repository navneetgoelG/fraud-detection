{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download_n_class_declr(data_path):\n",
    "    \n",
    "    # IMPORT LIBRARY \n",
    "    \n",
    "   \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "        \n",
    "    import random as python_random\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    # setting random seed for result reproducibility\n",
    "    np.random.seed(1)\n",
    "    python_random.seed(12)\n",
    "    \n",
    "    \n",
    "    # Data Download\n",
    "    credit_card_df = pd.read_csv('https://raw.github.com/HamoyeHQ/g01-fraud-detection/master/data/credit_card_dataset.zip')\n",
    "    \n",
    "    \n",
    "    print('=== DOWNLOAD DATA SUCCESSFUL ===')\n",
    "    \n",
    "    \n",
    "                       \n",
    "    # CREATING THE COLUMN SELECTOR CLASS\n",
    "                       \n",
    "    # 27 most important features according to our EDA\n",
    "    cols = ['V'+str(i) for i in range(1, 29) if i != 25]\n",
    "                       \n",
    "    class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, cols=cols):\n",
    "            self.cols = cols\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                return np.array(X[self.cols])\n",
    "\n",
    "            elif isinstance(X, pd.Series):\n",
    "                return np.array(X[self.cols]).reshape(1, -1)\n",
    "\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                self.cols_ind = [int(col[1:]) for col in self.cols]\n",
    "                if len(X.shape) == 1: # if one dimensional array\n",
    "                    return X[self.cols_ind].reshape(1, -1)\n",
    "                    return X[:, self.cols_ind]\n",
    "\n",
    "            else:\n",
    "                raise TypeError('expected input type to be any of pd.Series, pd.DataFrame or np.ndarray but got {}'.format(type(X)))\n",
    "            \n",
    "    print('=== CREATED COLUMN SELECTOR ===')\n",
    "\n",
    "                       \n",
    "    cols_select = ColumnSelector()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "  \n",
    "    print('=== SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(cols_select, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(scaler, f)\n",
    "                \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(credit_card_df, f)\n",
    "        \n",
    "    \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOWNLOAD DATA SUCCESSFUL ===\n",
      "=== CREATED COLUMN SELECTOR ===\n",
      "=== SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "### DATA DOWNLOAD ,FUNCTION AND CLASS DECLARATION\n",
    "data_download_n_class_declr(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_detection_model(data_path):\n",
    "    \n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    \n",
    "    import random as python_random\n",
    "    \n",
    "    import numpy as np\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    \n",
    "    \n",
    "    python_random.seed(12)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"rb\") as f:                \n",
    "        columnselector = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"rb\") as f:                \n",
    "        scaler = dill.load(f)\n",
    "            \n",
    "        \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"rb\") as f:                \n",
    "        data = dill.load(f)\n",
    "        \n",
    "        \n",
    "    print('=== DONE ===')\n",
    "        \n",
    "    \n",
    "    print('=== CREATING DATA PREPARATION PIPELINE ===')\n",
    "    # data preparation pipeline\n",
    "    data_prep = Pipeline([('columns', columnselector), ('scaler', scaler)])\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== CREATING FEATURES AND TARGETS ===')\n",
    "    \n",
    "    y = data.pop('Class')\n",
    "    X = data\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "\n",
    "    \n",
    "    print('=== FITTING DATA PREPARATION PIPELINE TO DATA ===')\n",
    "    \n",
    "\n",
    "    # fitting and transforming the data\n",
    "    X_prep = data_prep.fit_transform(X, y)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== CREATING FRAUD PREDICTING MODEL ===')\n",
    "    \n",
    "    model = XGBClassifier(random_state=1)\n",
    "    \n",
    "    admin_cost = 2.5\n",
    "    \n",
    "\n",
    "    sample_weights = np.array([X['Amount'].iloc[ind] if fraud else admin_cost for ind, fraud in enumerate(y.values)])\n",
    "    \n",
    "    print('=== FITTING DATA TO FRAUD PREDICTING MODEL ===')\n",
    "    \n",
    "    model.fit(X_prep, y, sample_weight=sample_weights);\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "\n",
    "    \n",
    "    print('=== CREATE AND FIT CALIBRATED CLASSIFIER TO PREPARED DATA ===')\n",
    "    calibration = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
    "    calibration.fit(X_prep, y);\n",
    "    print('=== DONE===')\n",
    "    \n",
    "    \n",
    "    print('=== SERIALIZING FUNCTIONS, FEATURES, TARGET, AND MODEL ===')\n",
    "    \n",
    "    # saving the data prep object\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'wb') as f:\n",
    "        dill.dump(data_prep, f)\n",
    "        \n",
    "    # saving the fitted calibrated classifier\n",
    "    with gzip.open(f\"{data_path}/calibrator.gz.dill\", 'wb') as f:\n",
    "        dill.dump(calibration, f)\n",
    "        \n",
    "    # saving the features\n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", 'wb') as f:\n",
    "        dill.dump(X, f)\n",
    "        \n",
    "    # saving the targets\n",
    "    with gzip.open(f\"{data_path}/target.gz.dill\", 'wb') as f:\n",
    "        dill.dump(y, f)\n",
    "        \n",
    "    \n",
    "              \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n",
      "=== CREATING DATA PREPARATION PIPELINE ===\n",
      "=== DONE ===\n",
      "=== CREATING FEATURES AND TARGETS ===\n",
      "=== DONE ===\n",
      "=== FITTING DATA PREPARATION PIPELINE TO DATA ===\n",
      "=== DONE ===\n",
      "=== CREATING FRAUD PREDICTING MODEL ===\n",
      "=== FITTING DATA TO FRAUD PREDICTING MODEL ===\n",
      "=== DONE ===\n",
      "=== CREATE AND FIT CALIBRATED CLASSIFIER TO PREPARED DATA ===\n",
      "=== DONE===\n",
      "=== SERIALIZING FUNCTIONS, FEATURES, TARGET, AND MODEL ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "fraud_detection_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(data_path):\n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    \n",
    "\n",
    "    import random as python_random\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    \n",
    "        \n",
    "    python_random.seed(12)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING FUNCTIONS, FEATURES, TARGET, AND MODEL ===')\n",
    "    \n",
    "    # loading in useful objects\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'rb') as f:\n",
    "        data_prep = dill.load(f)\n",
    "        \n",
    "    \n",
    "    with gzip.open(f\"{data_path}/calibrator.gz.dill\", 'rb') as f:\n",
    "        calibrator = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", \"rb\") as f:                \n",
    "        X = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/target.gz.dill\", \"rb\") as f:                \n",
    "        y = dill.load(f)\n",
    "        \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== DATA SPLITTING ===')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=1)\n",
    "        \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== TEST DATA PREPARATION ===')\n",
    "    \n",
    "    Xt = data_prep.transform(X_test)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== MODEL PREDICTION ===')\n",
    "    \n",
    "    def predictions(Xt):\n",
    "        if False:\n",
    "            pred = calibrator.predict_proba(Xt) # gets the probability of belonging to the positvie class\n",
    "            #print(f'This is from predict proba {pred}')\n",
    "\n",
    "            if len(pred.shape) > 1: # pred is 2-dim (multi-input)\n",
    "                pred = pred[:, 1]\n",
    "\n",
    "            else: # pred is 1-dim (single-input)\n",
    "                pred = pred[1]\n",
    "\n",
    "        else: # get raw predictions\n",
    "            pred = calibrator.predict(Xt) # gets the prediction\n",
    "            \n",
    "            #print(f'This is from predict {pred}')\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    predictions = predictions(Xt)\n",
    "\n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== SERIALIZING PREDICTIONS ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/predictions.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(predictions, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/testTargets.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(y_test, f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/testFeatures.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(X_test, f)\n",
    "    \n",
    "    \n",
    "    print(' === DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING FUNCTIONS, FEATURES, TARGET, AND MODEL ===\n",
      "=== DONE ===\n",
      "=== DATA SPLITTING ===\n",
      "=== DONE ===\n",
      "=== TEST DATA PREPARATION ===\n",
      "=== DONE ===\n",
      "=== MODEL PREDICTION ===\n",
      "=== DONE ===\n",
      "=== SERIALIZING PREDICTIONS ===\n",
      " === DONE ===\n"
     ]
    }
   ],
   "source": [
    "predictions(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_summary(data_path):\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    \n",
    "    \n",
    "    import random as python_random\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "        \n",
    "    python_random.seed(12)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING PREDICTIONS, TEST FEATURES, AND TEST TARGETS ===')\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/predictions.gz.dill\", 'rb') as f:\n",
    "        pred = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/testTargets.gz.dill\", 'rb') as f:\n",
    "        y_test = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/testFeatures.gz.dill\", \"rb\") as f:                \n",
    "        x_test = dill.load(f)\n",
    "        \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # defining a function to calculate cost savings\n",
    "    def cost_saving(ytrue, ypred, amount, threshold=0.5,admin_cost=2.5, epsilon=1e-7):\n",
    "        ypred = ypred.flatten()\n",
    "        fp = np.sum((ytrue == 0) & (ypred == 1))\n",
    "        cost = np.sum(fp*admin_cost) + np.sum((amount[(ytrue == 1) & (ypred == 0)]))\n",
    "        max_cost = np.sum((amount[(ytrue == 1)])) \n",
    "        savings = 1 - (cost/(max_cost+epsilon))\n",
    "        \n",
    "        return savings\n",
    "    \n",
    "\n",
    "\n",
    "    print('=== MULTI-INPUT TESTING ===')\n",
    "    \n",
    "    is_fraud = (pred >= 0.5).astype(np.int64)\n",
    "    pred_df = pd.DataFrame({'Class': is_fraud, 'Fraud_Probabilty': pred})\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('f1_score is {}'.format(f1_score(y_test, is_fraud)))\n",
    "    if isinstance(pred, np.ndarray):\n",
    "        amount = x_test.iloc[:, -1]\n",
    "    else:\n",
    "        amount = x_test.iloc[:, -1]\n",
    "    print('cost saving is {}'.format(cost_saving(y_test, is_fraud, amount)))\n",
    "    \n",
    "    print(pred_df.head())\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== SINGLE-INPUT TESTING ===')\n",
    "    \n",
    "    \n",
    "    is_fraud2 = (pred[0] >= 0.5).astype(np.int64)\n",
    "    \n",
    "    pred_df2 = pd.DataFrame({'Class': is_fraud2, 'Fraud_Probabilty': pred[0]}, index=[0])\n",
    "\n",
    "    print('f1_score is {}'.format(f1_score([y_test[0]], [is_fraud2])))#.format(f1_score(y_test[0], is_fraud2)))\n",
    "    print('cost saving is {}'.format(cost_saving(y_test.iloc[0], is_fraud2, x_test.iloc[0][-1].reshape(1))))\n",
    "    \n",
    "    print(pred_df2)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== SERIALZING RESULTS ===')\n",
    "    # write predictions to results.txt\n",
    "    with open(f'{data_path}/results.txt','w') as result:\n",
    "        result.write(f'MULTI-INPUT TESTING:\\n F1 SCORE: {f1_score(y_test, is_fraud)} \\n COST SAVING: {cost_saving(y_test, is_fraud, amount)} \\n {pred_df.head()} \\n \\n SINGLE-INPUT TESTING:\\n F1 SCORE: {f1_score([y_test[0]], [is_fraud2])} \\n COST SAVING: {cost_saving(y_test.iloc[0], is_fraud2, x_test.iloc[0][-1].reshape(1))} \\n {pred_df2.head()}')\n",
    "        \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING PREDICTIONS, TEST FEATURES, AND TEST TARGETS ===\n",
      "=== DONE ===\n",
      "=== MULTI-INPUT TESTING ===\n",
      "f1_score is 0.8897959183673471\n",
      "cost saving is 0.9624808839300956\n",
      "   Class  Fraud_Probabilty\n",
      "0      0                 0\n",
      "1      0                 0\n",
      "2      0                 0\n",
      "3      0                 0\n",
      "4      0                 0\n",
      "=== DONE ===\n",
      "=== SINGLE-INPUT TESTING ===\n",
      "f1_score is 0.0\n",
      "cost saving is 1.0\n",
      "   Class  Fraud_Probabilty\n",
      "0      0                 0\n",
      "=== DONE ===\n",
      "=== SERIALZING RESULTS ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "prediction_summary(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download_n_class_declr_op = comp.func_to_container_op(data_download_n_class_declr, base_image= \"python:3.7\")\n",
    "fraud_detection_model_op = comp.func_to_container_op(fraud_detection_model, base_image= \"python:3.7\")\n",
    "predictions_op = comp.func_to_container_op(predictions, base_image=\"python:3.7\")\n",
    "prediction_summary_op = comp.func_to_container_op(prediction_summary, base_image=\"python:3.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name =\"Fraud Detection\",\n",
    "        description = \"Fraud Detection Pipeline\")\n",
    "\n",
    "def fraud_detection(data_path:str):\n",
    "    \n",
    "    volume_op = dsl.VolumeOp(\n",
    "        name=\"data_volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create data download components.\n",
    "    data_download_class_declr_container = data_download_n_class_declr_op(data_path).add_pvolumes({data_path:volume_op.volume})\n",
    "\n",
    "    # Create data preprocessing component.\n",
    "    fraud_detection_model_container = fraud_detection_model_op(data_path).add_pvolumes({data_path: data_download_class_declr_container.pvolume})\n",
    "        \n",
    "    # Create Predictions Component.\n",
    "    predictions_container = predictions_op(data_path)\\\n",
    "                                        .add_pvolumes({data_path:fraud_detection_model_container.pvolume})\n",
    "    \n",
    "    # Prediction Summary Component.\n",
    "    prediction_summary_container = prediction_summary_op(data_path)\\\n",
    "                                        .add_pvolumes({data_path:predictions_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    result_container = dsl.ContainerOp(\n",
    "            name=\"print_prediction\",\n",
    "            image='library/bash:4.4.23',\n",
    "            pvolumes={data_path: prediction_summary_container.pvolume},\n",
    "            arguments=['cat', f'{data_path}/results.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c9ced072-91e6-45d6-b827-cd1a72cc3a19\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e525dd24-925c-403c-bbda-f59a5923e4dd\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = \"/mnt\"\n",
    "\n",
    "pipeline_func = fraud_detection\n",
    "\n",
    "experiment_name = 'fraud_detection_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
