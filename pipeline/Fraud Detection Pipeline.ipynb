{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip\n",
    "#!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy scikit-learn==0.22 tensorflow==2.3 keras==2.4.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download_n_class_declr(data_path):\n",
    "    \n",
    "    # IMPORT LIBRARY \n",
    "    \n",
    "   \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import random as python_random\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    # setting random seed for result reproducibility\n",
    "    np.random.seed(1)\n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    # Data Download\n",
    "    credit_card_df = pd.read_csv('https://raw.github.com/HamoyeHQ/g01-fraud-detection/master/data/credit_card_dataset.zip')\n",
    "    \n",
    "    \n",
    "    print('=== DOWNLOAD DATA SUCCESSFUL ===')\n",
    "    \n",
    "    \n",
    "                       \n",
    "    # CREATING THE COLUMN SELECTOR CLASS\n",
    "                       \n",
    "    # 27 most important features according to our EDA\n",
    "    cols = ['V'+str(i) for i in range(1, 29) if i != 25]\n",
    "                       \n",
    "    class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, cols=cols):\n",
    "            self.cols = cols\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                return np.array(X[self.cols])\n",
    "\n",
    "            elif isinstance(X, pd.Series):\n",
    "                return np.array(X[self.cols]).reshape(1, -1)\n",
    "\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                self.cols_ind = [int(col[1:]) for col in self.cols]\n",
    "                if len(X.shape) == 1: # if one dimensional array\n",
    "                    return X[self.cols_ind].reshape(1, -1)\n",
    "                    return X[:, self.cols_ind]\n",
    "\n",
    "            else:\n",
    "                raise TypeError('expected input type to be any of pd.Series, pd.DataFrame or np.ndarray but got {}'.format(type(X)))\n",
    "            \n",
    "    print('=== CREATED COLUMN SELECTOR ===')\n",
    "\n",
    "    # CREATING THE OUTLIERS CLIPPER CLASS                   \n",
    "    class ClipOutliers(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "            self.lower_percentile = lower_percentile\n",
    "            self.upper_percentile = upper_percentile\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.a = np.percentile(X, self.lower_percentile, axis=0)\n",
    "            self.b = np.percentile(X, self.upper_percentile, axis=0)\n",
    "\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            self.Xt = np.clip(X, self.a, self.b)\n",
    "\n",
    "            return self.Xt\n",
    "        \n",
    "    print('=== CREATED OUTLIER CLIPPER ===')\n",
    "                       \n",
    "    cols_select = ColumnSelector()\n",
    "    scaler = StandardScaler()\n",
    "    clipper = ClipOutliers()\n",
    "  \n",
    "    print('=== SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(cols_select, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(scaler, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/clipper.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(clipper, f)\n",
    "\n",
    "                \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(credit_card_df, f)\n",
    "        \n",
    "    \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOWNLOAD DATA SUCCESSFUL ===\n",
      "=== CREATED COLUMN SELECTOR ===\n",
      "=== CREATED OUTLIER CLIPPER ===\n",
      "=== SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "### DATA DOWNLOAD ,FUNCTION AND CLASS DECLARATION\n",
    "data_download_n_class_declr(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_sensitive_model(data_path):\n",
    "    \n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import random as python_random\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    \n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"rb\") as f:                \n",
    "        columnselector = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"rb\") as f:                \n",
    "        scaler = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/clipper.gz.dill\", \"rb\") as f:                \n",
    "        clipper = dill.load(f)\n",
    "        \n",
    "        \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"rb\") as f:                \n",
    "        data = dill.load(f)\n",
    "        \n",
    "        \n",
    "    print('=== DONE ===')\n",
    "        \n",
    "    # setting _estimator_type atrribute of sklearn's pipeline to 'classifier' to avoid errors when using\n",
    "    # VotingClassifier.\n",
    "    class ClassifierPipeline(Pipeline):\n",
    "        @property\n",
    "        def _estimator_type(self):\n",
    "            return 'classifier'\n",
    "        \n",
    "\n",
    "                       \n",
    "    # BUILDING THE MLP MODEL FUNCTION\n",
    "    \n",
    "    y = data['Class']\n",
    "    neg, pos = np.bincount(y)\n",
    "    initial_bias = np.log([pos/neg])\n",
    "    \n",
    "    print('=== BUILD MLP NETWORK ===')\n",
    "                       \n",
    "    def build_model():\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        output_bias = tf.keras.initializers.Constant(initial_bias) \n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "        # compling model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== CREATING VOTING ENSEMBLE MODEL ===')\n",
    "    \n",
    "    epochs = 4\n",
    "    n_neighbors = 5\n",
    "    \n",
    "    \n",
    "    mlp = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=512, verbose=0) # model 1\n",
    "    knn =  KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree', n_jobs=4) # model 2\n",
    "\n",
    "    clip_mlp = ClassifierPipeline([('clipper', clipper), ('mlp', mlp)]) # model 1 requires clipping, so it is encapsulated in a pipeline with a clipper\n",
    "\n",
    "    vote_ensemble = VotingClassifier(estimators=[('knn', knn), ('mlp', clip_mlp)], voting='soft') # voting ensemble\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "\n",
    "    print('=== CREATING DATA PREPARATION PIPELINE ===')\n",
    "    # data preparation pipeline\n",
    "    data_prep = Pipeline([('columns', columnselector), ('scaler', scaler)])\n",
    "    \n",
    "    \n",
    "    print('=== FITTING DATA PREPARATION PIPELINE TO DATA ===')\n",
    "    \n",
    "    y = data.pop('Class')\n",
    "    X = data\n",
    "    \n",
    "    # fitting and transforming the data\n",
    "    X_prep = data_prep.fit_transform(X, y)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== FITTING VOTE ENSEMBLED MODEL TO  PREPARED DATA ===')\n",
    "    vote_ensemble.fit(X_prep, y); # fitting the voting ensemble\n",
    "    print('=== DONE===')\n",
    "    \n",
    "    \n",
    "    print('=== SERIALIZING FUNCTIONS AND MODELS ===')\n",
    "    \n",
    "    # saving the data prep object\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'wb') as f:\n",
    "        dill.dump(data_prep, f)\n",
    "        \n",
    "    # saving the fitted knn model\n",
    "    #with gzip.open(f\"{data_path}/knn.gz.dill\", 'wb') as f:\n",
    "     #   dill.dump(vote_ensemble.estimators_[0], f)\n",
    "        \n",
    "    # saving the clipper2 object\n",
    "    with gzip.open(f\"{data_path}/clipper2.gz.dill\", 'wb') as f:\n",
    "        dill.dump(vote_ensemble.estimators_[1][0], f)\n",
    "\n",
    "    # saving the features \n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", 'wb') as f:\n",
    "        dill.dump(X, f)\n",
    "    \n",
    "     # saving the targets\n",
    "    with gzip.open(f\"{data_path}/targets.gz.dill\", 'wb') as f:\n",
    "        dill.dump(y, f)\n",
    "        \n",
    "    # saving the transformed data\n",
    "    with gzip.open(f\"{data_path}/X_prep.gz.dill\", 'wb') as f:\n",
    "        dill.dump(X_prep, f)\n",
    "        \n",
    "        \n",
    "   # saving the label encoder object of the voting ensemble\n",
    "    with gzip.open(f\"{data_path}/label_encoder.gz.dill\", 'wb') as f:\n",
    "        dill.dump(vote_ensemble.le_, f)\n",
    "        \n",
    "    vote_ensemble.estimators_[1][1].model.save(f'{data_path}/mlp.h5') # saving the mlp model\n",
    "    \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n",
      "=== BUILD MLP NETWORK ===\n",
      "=== DONE ===\n",
      "=== CREATING VOTING ENSEMBLE MODEL ===\n",
      "=== DONE ===\n",
      "=== CREATING DATA PREPARATION PIPELINE ===\n",
      "=== FITTING DATA PREPARATION PIPELINE TO DATA ===\n",
      "=== DONE ===\n",
      "=== FITTING VOTE ENSEMBLED MODEL TO  PREPARED DATA ===\n",
      "=== DONE===\n",
      "=== SERIALIZING FUNCTIONS AND MODELS ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "fraud_sensitive_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(data_path):\n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import random as python_random\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "        \n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING CLASSESS, AND DATA ===')\n",
    "    \n",
    "    # loading in useful objects\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'rb') as f:\n",
    "        data_prep = dill.load(f)\n",
    "\n",
    "    #with gzip.open(f\"{data_path}/knn.gz.dill\", 'rb') as f:\n",
    "    #    knn = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/clipper2.gz.dill\", 'rb') as f:\n",
    "        clipper2 = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/label_encoder.gz.dill\", 'rb') as f:\n",
    "        le = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", 'rb') as f:\n",
    "        X = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/targets.gz.dill\", 'rb') as f:\n",
    "        y = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/X_prep.gz.dill\", 'rb') as f:\n",
    "        X_prep = dill.load(f)\n",
    "        \n",
    "        \n",
    "    build_model = lambda: load_model(f\"{data_path}/mlp.h5\") # loading in the mlp model\n",
    "\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== FITTING THE TRANSFORMED DATA TO THE KNN MODEL ===')\n",
    "    \n",
    "    n_neighbors = 5\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree', n_jobs=4)\n",
    "    knn.fit(X_prep, y)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "\n",
    "    # setting useful atrributes and parameters\n",
    "    classes = np.array([0, 1])\n",
    "    epochs = 4\n",
    "    batch_size = 512\n",
    "\n",
    "    print('=== INITIALIZE THE MLP MODEL ===')\n",
    "    \n",
    "    # setting _estimator_type atrribute of sklearn's pipeline to 'classifier' to avoid errors when using\n",
    "    # VotingClassifier.\n",
    "    class ClassifierPipeline(Pipeline):\n",
    "        @property\n",
    "        def _estimator_type(self):\n",
    "            return 'classifier'\n",
    "        \n",
    "    # initializes the mlp model\n",
    "    mlp = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    mlp.model = build_model() # rebuilding the mlp model\n",
    "    mlp.classes_ = classes # setting the classes_ attribute of the mlp model\n",
    "\n",
    "    clip_mlp = ClassifierPipeline([('clipper2', clipper2), ('mlp', mlp)]) # clipping pipeline\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== RECONSTRUCTING THE VOTING CLASSIFIER ===')\n",
    "\n",
    "    # reconstructing the voting classifier\n",
    "    vote_ensemble = VotingClassifier(estimators=[('knn', knn), ('mlp', clip_mlp)], voting='soft')\n",
    "    vote_ensemble.classes_ = classes\n",
    "    vote_ensemble.estimators_ = [knn, clip_mlp]\n",
    "    vote_ensemble.le_ = le\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== DATA SPLITTING AND PREPARATION ===')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=1)\n",
    "    \n",
    "    Xt = data_prep.transform(X_test)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== MODEL PREDICTION ===')\n",
    "    \n",
    "    def predictions(Xt):\n",
    "        if True:\n",
    "            pred = vote_ensemble.predict_proba(Xt) # gets the probability of belonging to the positvie class\n",
    "            #print(f'This is from predict proba {pred}')\n",
    "\n",
    "            if len(pred.shape) > 1: # pred is 2-dim (multi-input)\n",
    "                pred = pred[:, 1]\n",
    "\n",
    "            else: # pred is 1-dim (single-input)\n",
    "                pred = pred[1]\n",
    "\n",
    "        else: # get raw predictions\n",
    "            pred = vote_ensemble.predict(Xt) # gets the prediction\n",
    "            \n",
    "            #print(f'This is from predict {pred}')\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    predictions = predictions(Xt)\n",
    "\n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== SERIALIZING PREDICTIONS, TEST FEATURES AND TEST TARGETS ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/predictions.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(predictions, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/testTargets.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(y_test, f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/testFeatures.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(X_test, f)\n",
    "    \n",
    "    print(' === DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n",
      "=== FITTING THE TRANSFORMED DATA TO THE KNN MODEL ===\n",
      "=== DONE ===\n",
      "=== INITIALIZE THE MLP MODEL ===\n",
      "=== DONE ===\n",
      "=== RECONSTRUCTING THE VOTING CLASSIFIER ===\n",
      "=== DONE ===\n",
      "=== DATA SPLITTING AND PREPARATION ===\n",
      "=== DONE ===\n",
      "=== MODEL PREDICTION ===\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py:264: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "=== DONE ===\n",
      "=== SERIALIZING PREDICTIONS, TEST FEATURES AND TEST TARGETS ===\n",
      " === DONE ===\n"
     ]
    }
   ],
   "source": [
    "train_predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_summary(data_path):\n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import random as python_random\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "        \n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING PREDICTIONS, TEST FEATURES, AND TEST TARGETS ===')\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/predictions.gz.dill\", 'rb') as f:\n",
    "        pred = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/testTargets.gz.dill\", 'rb') as f:\n",
    "        y_test = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/testFeatures.gz.dill\", \"rb\") as f:                \n",
    "        x_test = dill.load(f)\n",
    "        \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # defining a function to calculate cost savings\n",
    "    def cost_saving(ytrue, ypred, amount, threshold=0.5,admin_cost=2.5, epsilon=1e-7):\n",
    "        ypred = ypred.flatten()\n",
    "        fp = np.sum((ytrue == 0) & (ypred == 1))\n",
    "        cost = np.sum(fp*admin_cost) + np.sum((amount[(ytrue == 1) & (ypred == 0)]))\n",
    "        max_cost = np.sum((amount[(ytrue == 1)])) \n",
    "        savings = 1 - (cost/(max_cost+epsilon))\n",
    "        \n",
    "        return savings\n",
    "    \n",
    "\n",
    "\n",
    "    print('=== MULTI-INPUT TESTING ===')\n",
    "    \n",
    "    is_fraud = (pred >= 0.5).astype(np.int64)\n",
    "    pred_df = pd.DataFrame({'Class': is_fraud, 'Fraud_Probabilty': pred})\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('f1_score is {}'.format(f1_score(y_test, is_fraud)))\n",
    "    if isinstance(pred, np.ndarray):\n",
    "        amount = x_test.iloc[:, -1]\n",
    "    else:\n",
    "        amount = x_test.iloc[:, -1]\n",
    "    print('cost saving is {}'.format(cost_saving(y_test, is_fraud, amount)))\n",
    "    \n",
    "    print(pred_df.head())\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== SINGLE-INPUT TESTING ===')\n",
    "    \n",
    "    \n",
    "    is_fraud2 = (pred[0] >= 0.5).astype(np.int64)\n",
    "    \n",
    "    pred_df2 = pd.DataFrame({'Class': is_fraud2, 'Fraud_Probabilty': pred[0]}, index=[0])\n",
    "    \n",
    "    print(y_test[0])\n",
    "    print(is_fraud2)\n",
    "\n",
    "    print('f1_score is {}'.format(f1_score([y_test[0]], [is_fraud2])))#.format(f1_score(y_test[0], is_fraud2)))\n",
    "    print('cost saving is {}'.format(cost_saving(y_test.iloc[0], is_fraud2, x_test.iloc[0][-1].reshape(1))))\n",
    "    \n",
    "    print(pred_df2)\n",
    "    \n",
    "    print('=== DONE ===')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING PREDICTIONS, TEST FEATURES, AND TEST TARGETS ===\n",
      "=== DONE ===\n",
      "=== MULTI-INPUT TESTING ===\n",
      "f1_score is 0.8945147679324894\n",
      "cost saving is 0.7310398178640722\n",
      "   Class  Fraud_Probabilty\n",
      "0      0          0.000037\n",
      "1      0          0.000278\n",
      "2      0          0.000022\n",
      "3      0          0.000044\n",
      "4      0          0.000017\n",
      "=== DONE ===\n",
      "=== SINGLE-INPUT TESTING ===\n",
      "0\n",
      "0\n",
      "f1_score is 0.0\n",
      "cost saving is 1.0\n",
      "   Class  Fraud_Probabilty\n",
      "0      0          0.000037\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "prediction_summary(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download_n_class_declr_op = comp.func_to_container_op(data_download_n_class_declr, base_image= \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "fraud_sensitive_model_op = comp.func_to_container_op(fraud_sensitive_model, base_image= \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_predict_op = comp.func_to_container_op(train_predict, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "prediction_summary_op = comp.func_to_container_op(prediction_summary, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name =\"Fraud Detection\",\n",
    "        description = \"Fraud Detection Pipeline\")\n",
    "\n",
    "def fraud_detection(data_path:str):\n",
    "    \n",
    "    volume_op = dsl.VolumeOp(\n",
    "        name=\"data_volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create data download components.\n",
    "    data_download_class_declr_container = data_download_n_class_declr_op(data_path).add_pvolumes({data_path:volume_op.volume})\n",
    "\n",
    "    # Create data preprocessing component.\n",
    "    fraud_sensitive_model_container = fraud_sensitive_model_op(data_path).add_pvolumes({data_path: data_download_class_declr_container.pvolume})\n",
    "        \n",
    "    # Create Forecasting Component.\n",
    "    forecasting_conatiner = train_predict_op(data_path)\\\n",
    "                                        .add_pvolumes({data_path:fraud_sensitive_model_container.pvolume})\n",
    "    \n",
    "    # Prediction Summary Component.\n",
    "    prediction_summary_conatiner = prediction_summary_op(data_path)\\\n",
    "                                        .add_pvolumes({data_path:forecasting_conatiner.pvolume})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/16d623ec-3c08-445d-a958-947cd81dadcc\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/26300163-b40f-4bcc-85e1-baf62cd1b7d2\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DATA_PATH =\"/home/jovyan/data/\"\n",
    "DATA_PATH = \"/mnt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline_func = fraud_detection\n",
    "\n",
    "experiment_name = 'fraud_detection_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
