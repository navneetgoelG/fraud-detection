{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip\n",
    "#!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy scikit-learn==0.22 tensorflow==2.3 keras==2.4.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for pipeline\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download_n_class_declr(data_path):\n",
    "    \n",
    "    # IMPORT LIBRARY \n",
    "    \n",
    "   \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    import random as python_random\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    # setting random seed for result reproducibility\n",
    "    np.random.seed(1)\n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    # Data Download\n",
    "    credit_card_df = pd.read_csv('https://raw.github.com/HamoyeHQ/g01-fraud-detection/master/data/credit_card_dataset.zip')\n",
    "    \n",
    "    \n",
    "    print('=== DOWNLOAD DATA SUCCESSFUL ===')\n",
    "    \n",
    "                       \n",
    "    # CREATING THE COLUMN SELECTOR CLASS\n",
    "                       \n",
    "    # 27 most important features according to our EDA\n",
    "    cols = ['V'+str(i) for i in range(1, 29) if i != 25]\n",
    "                       \n",
    "    class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "        def __init__(self, cols=cols):\n",
    "            self.cols = cols\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                return np.array(X[self.cols])\n",
    "                \n",
    "            elif isinstance(X, pd.Series):\n",
    "                return np.array(X[self.cols]).reshape(1, -1)\n",
    "\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                self.cols_ind = [int(col[1:]) for col in self.cols]\n",
    "                return X[:, self.cols_ind]\n",
    "                \n",
    "            else:\n",
    "                raise TypeError('expected input type to be any of pd.Series, pd.DataFrame or np.ndarray but got {}'.format(type(X)))\n",
    "\n",
    "    print('=== CREATED COLUMN SELECTOR ===')\n",
    "        \n",
    "    # CREATING THE OUTLIERS CLIPPER CLASS                   \n",
    "    class ClipOutliers(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "            self.lower_percentile = lower_percentile\n",
    "            self.upper_percentile = upper_percentile\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.a = np.percentile(X, self.lower_percentile, axis=0)\n",
    "            self.b = np.percentile(X, self.upper_percentile, axis=0)\n",
    "\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            self.Xt = np.clip(X, self.a, self.b)\n",
    "\n",
    "            return self.Xt\n",
    "        \n",
    "    print('=== CREATED OUTLIER CLIPPER ===')\n",
    "                       \n",
    "    cols_select = ColumnSelector()\n",
    "    scaler = StandardScaler()\n",
    "    clipper = ClipOutliers()\n",
    "  \n",
    "    print('=== SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(cols_select, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(scaler, f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/clipper.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(clipper, f)\n",
    "        \n",
    "        \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"wb\") as f:                \n",
    "        dill.dump(credit_card_df, f)\n",
    "        \n",
    "    \n",
    "    print('=== DONE ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOWNLOAD DATA SUCCESSFUL ===\n",
      "=== CREATED COLUMN SELECTOR ===\n",
      "=== CREATED OUTLIER CLIPPER ===\n",
      "=== SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "### DATA DOWNLOAD ,FUNCTION AND CLASS DECLARATION\n",
    "data_download_n_class_declr(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_sensitive_model(data_path):\n",
    "    \n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import random as python_random\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    \n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING CLASSESS, AND DATA ===')\n",
    "                       \n",
    "    with gzip.open(f\"{data_path}/columnSelector.gz.dill\", \"rb\") as f:                \n",
    "        columnselector = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/scaler.gz.dill\", \"rb\") as f:                \n",
    "        scaler = dill.load(f)\n",
    "    \n",
    "    with gzip.open(f\"{data_path}/clipper.gz.dill\", \"rb\") as f:                \n",
    "        clipper = dill.load(f)\n",
    "        \n",
    "        \n",
    "    with gzip.open(f\"{data_path}/data.gz.dill\", \"rb\") as f:                \n",
    "        data = dill.load(f)\n",
    "        \n",
    "        \n",
    "    print('=== DONE ===')\n",
    "        \n",
    "    # setting _estimator_type atrribute of sklearn's pipeline to 'classifier' to avoid errors when using\n",
    "    # VotingClassifier.\n",
    "    class ClassifierPipeline(Pipeline):\n",
    "        @property\n",
    "        def _estimator_type(self):\n",
    "            return 'classifier'\n",
    "        \n",
    "        \n",
    "    \n",
    "    epochs = 4\n",
    "    n_neighbors = 5\n",
    "    \n",
    "    \n",
    "    \n",
    "                       \n",
    "    # BUILDING THE MLP MODEL FUNCTION\n",
    "    \n",
    "    y = data['Class']\n",
    "    neg, pos = np.bincount(y)\n",
    "    initial_bias = np.log([pos/neg])\n",
    "    \n",
    "    print('=== BUILD MLP NETWORK ===')\n",
    "                       \n",
    "    def build_model():\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        output_bias = tf.keras.initializers.Constant(initial_bias) \n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "        # compling model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('=== CREATING VOTING ENSEMBLE MODEL ===')\n",
    "    mlp = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=512, verbose=0) # model 1\n",
    "    knn =  KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree', n_jobs=4) # model 2\n",
    "\n",
    "    clip_mlp = ClassifierPipeline([('clipper', clipper), ('mlp', mlp)]) # model 1 requires clipping, so it is encapsulated in a pipeline with a clipper\n",
    "\n",
    "    vote_ensemble = VotingClassifier(estimators=[('knn', knn), ('mlp', clip_mlp)], voting='soft') # voting ensemble\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "\n",
    "    print('=== CREATING DATA PREPARATION PIPELINE ===')\n",
    "    # data preparation pipeline\n",
    "    data_prep = Pipeline([('columns', columnselector), ('scaler', scaler)])\n",
    "    \n",
    "    \n",
    "    print('=== FITTING DATA PREPARATION PIPELINE TO DATA ===')\n",
    "    \n",
    "    y = data.pop('Class')\n",
    "    X = data\n",
    "    \n",
    "    # fitting and transforming the data\n",
    "    X_prep = data_prep.fit_transform(X, y)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== FITTING VOTE ENSEMBLED MODEL TO  PREPARED DATA ===')\n",
    "    vote_ensemble.fit(X_prep, y); # fitting the voting ensemble\n",
    "    print('=== DONE===')\n",
    "    \n",
    "    \n",
    "    print('=== SERIALIZING FUNCTIONS AND MODELS ===')\n",
    "    \n",
    "    # saving the data prep object\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'wb') as f:\n",
    "        dill.dump(data_prep, f)\n",
    "        \n",
    "    # saving the fitted knn model\n",
    "    with gzip.open(f\"{data_path}/knn.gz.dill\", 'wb') as f:\n",
    "        dill.dump(vote_ensemble.estimators_[0], f)\n",
    "        \n",
    "    # saving the clipper2 object\n",
    "    with gzip.open(f\"{data_path}/clipper2.gz.dill\", 'wb') as f:\n",
    "        dill.dump(vote_ensemble.estimators_[1][0], f)\n",
    "\n",
    "    # saving the features \n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", 'wb') as f:\n",
    "        dill.dump(X, f)\n",
    "    \n",
    "     # saving the targets\n",
    "    with gzip.open(f\"{data_path}/targets.gz.dill\", 'wb') as f:\n",
    "        dill.dump(y, f)\n",
    "        \n",
    "        \n",
    "    \n",
    "   # saving the label encoder object of the voting ensemble\n",
    "    with gzip.open(f\"{data_path}/label_encoder.gz.dill\", 'wb') as f:\n",
    "        dill.dump(vote_ensemble.le_, f)\n",
    "        \n",
    "    vote_ensemble.estimators_[1][1].model.save(f'{data_path}/mlp.h5') # saving the mlp model\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n",
      "=== BUILD MLP NETWORK ===\n",
      "=== DONE ===\n",
      "=== CREATING VOTING ENSEMBLE MODEL ===\n",
      "=== DONE ===\n",
      "=== CREATING DATA PREPARATION PIPELINE ===\n",
      "=== FITTING DATA PREPARATION PIPELINE TO DATA ===\n",
      "=== DONE ===\n",
      "=== FITTING VOTE ENSEMBLED MODEL TO  PREPARED DATA ===\n",
      "=== DONE===\n",
      "=== SERIALIZING FUNCTIONS AND MODELS ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "# CREATING THE FRAUD SENSITIVE MODEL\n",
    "fraud_sensitive_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(data_path):\n",
    "    \n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"dill\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn==0.22\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import random as python_random\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from sklearn.model_selection import  train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "        \n",
    "    python_random.seed(12)\n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    \n",
    "    import dill\n",
    "    import gzip\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    \n",
    "    print('=== DE-SERIALIZING CLASSESS, AND DATA ===')\n",
    "    \n",
    "    # loading in useful objects\n",
    "    with gzip.open(f\"{data_path}/data_prep_pipe.gz.dill\", 'rb') as f:\n",
    "        data_prep = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/knn.gz.dill\", 'rb') as f:\n",
    "        knn = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/clipper2.gz.dill\", 'rb') as f:\n",
    "        clipper2 = dill.load(f)\n",
    "\n",
    "    with gzip.open(f\"{data_path}/label_encoder.gz.dill\", 'rb') as f:\n",
    "        le = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/features.gz.dill\", 'rb') as f:\n",
    "        X = dill.load(f)\n",
    "        \n",
    "    with gzip.open(f\"{data_path}/targets.gz.dill\", 'rb') as f:\n",
    "        y = dill.load(f)\n",
    "    \n",
    "    build_model = lambda: load_model(f\"{data_path}/mlp.h5\") # loading in the mlp model\n",
    "\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "\n",
    "    # setting useful atrributes and parameters\n",
    "    classes = np.array([0, 1])\n",
    "    epochs = 4\n",
    "    batch_size = 512\n",
    "\n",
    "    print('=== INITIALIZE THE MLP MODEL ===')\n",
    "    \n",
    "    # setting _estimator_type atrribute of sklearn's pipeline to 'classifier' to avoid errors when using\n",
    "    # VotingClassifier.\n",
    "    class ClassifierPipeline(Pipeline):\n",
    "        @property\n",
    "        def _estimator_type(self):\n",
    "            return 'classifier'\n",
    "        \n",
    "    # initializes the mlp model\n",
    "    mlp = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    mlp.model = build_model() # rebuilding the mlp model\n",
    "    mlp.classes_ = classes # setting the classes_ attribute of the mlp model\n",
    "\n",
    "    clip_mlp = ClassifierPipeline([('clipper2', clipper2), ('mlp', mlp)]) # clipping pipeline\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    print('=== RECONSTRUCTING THE VOTING CLASSIFIER ===')\n",
    "\n",
    "    # reconstructing the voting classifier\n",
    "    vote_ensemble = VotingClassifier(estimators=[('knn', knn), ('mlp', clip_mlp)], voting='soft')\n",
    "    vote_ensemble.classes_ = classes\n",
    "    vote_ensemble.estimators_ = [knn, clip_mlp]\n",
    "    vote_ensemble.le_ = le\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== DATA SPLITTING AND PREPARATION ===')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=1)\n",
    "    \n",
    "    X_prep = data_prep.transform(X_test)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== MODEL PREDICTION ===')\n",
    "    \n",
    "    pred = vote_ensemble.predict(X_prep)\n",
    "    \n",
    "    print('=== DONE ===')\n",
    "    \n",
    "    \n",
    "    print('=== F1-SCORE ===')\n",
    "    score = f1_score(y_test, pred)\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    print(' === DONE ===')\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DE-SERIALIZING CLASSESS, AND DATA ===\n",
      "=== DONE ===\n",
      "=== INITIALIZE THE MLP MODEL ===\n",
      "=== DONE ===\n",
      "=== RECONSTRUCTING THE VOTING CLASSIFIER ===\n",
      "=== DONE ===\n",
      "=== DATA SPLITTING AND PREPARATION ===\n",
      "=== DONE ===\n",
      "=== MODEL PREDICTION ===\n",
      "=== DONE ===\n",
      "=== F1-SCORE ===\n",
      "0.8945147679324894\n",
      " === DONE ===\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AND PREDICTION\n",
    "train_predict(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download_n_class_declr_op = comp.func_to_container_op(data_download_n_class_declr, base_image= \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "fraud_sensitive_model_op = comp.func_to_container_op(fraud_sensitive_model, base_image= \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_predict_op = comp.func_to_container_op(train_predict, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name =\"Fraud Detection\",\n",
    "        description = \"Fraud Detection Pipeline\")\n",
    "\n",
    "def fraud_detection(data_path:str):\n",
    "    \n",
    "    volume_op = dsl.VolumeOp(\n",
    "        name=\"data_volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create data download components.\n",
    "    data_download_class_declr_container = data_download_n_class_declr_op(data_path).add_pvolumes({data_path:volume_op.volume})\n",
    "\n",
    "    # Create data preprocessing component.\n",
    "    fraud_sensitive_model_container = fraud_sensitive_model_op(data_path).add_pvolumes({data_path: data_download_class_declr_container.pvolume})\n",
    "        \n",
    "    # Create Forecasting Component.\n",
    "    forecasting_conatiner = train_predict_op(data_path)\\\n",
    "                                        .add_pvolumes({data_path:fraud_sensitive_model_container.pvolume})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/b9597b0e-6b20-4c09-b33a-78597f7296cc\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/719b88c9-cc85-4160-a653-54539d4a06a9\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DATA_PATH =\"/home/jovyan/data/\"\n",
    "DATA_PATH = \"/mnt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline_func = fraud_detection\n",
    "\n",
    "experiment_name = 'fraud_detection_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
